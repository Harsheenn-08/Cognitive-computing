{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74b235a7-a2ab-4595-9307-ef1148b5716e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'technology': 2, 'love': 1, 'reading': 1, 'books': 1, 'provide': 1, 'great': 1, 'way': 1, 'learn': 1, 'new': 1, 'advancements': 1, 'ideas': 1, 'made': 1, 'lives': 1, 'easier': 1, 'connected': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\HARSHEEN\n",
      "[nltk_data]     KAUR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize , sent_tokenize \n",
    "from collections import Counter\n",
    "\n",
    "text = \"I love reading books about technology. They provide a great way to learn about new advancements and ideas. Technology has made our lives easier and more connected.\"\n",
    "text = text.lower()\n",
    "text=text.translate(str.maketrans('','',string.punctuation))\n",
    "words=word_tokenize(text)\n",
    "nltk.download('stopwords')\n",
    "stop_words=set(stopwords.words('english'))\n",
    "filtered_words=[word for word in words if word not in stop_words]\n",
    "word_freq=Counter(filtered_words)\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "504e41a9-0291-47a9-841f-478ac6f81fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['love', 'reading', 'books', 'technology', 'provide', 'great', 'way', 'learn', 'new', 'advancements', 'ideas', 'technology', 'made', 'lives', 'easier', 'connected']\n",
      "Porter Stemmed: ['love', 'read', 'book', 'technolog', 'provid', 'great', 'way', 'learn', 'new', 'advanc', 'idea', 'technolog', 'made', 'live', 'easier', 'connect']\n",
      "Lancaster Stemmed: ['lov', 'read', 'book', 'technolog', 'provid', 'gre', 'way', 'learn', 'new', 'adv', 'idea', 'technolog', 'mad', 'liv', 'easy', 'connect']\n",
      "Lemmatized: ['love', 'reading', 'book', 'technology', 'provide', 'great', 'way', 'learn', 'new', 'advancement', 'idea', 'technology', 'made', 'life', 'easier', 'connected']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\HARSHEEN\n",
      "[nltk_data]     KAUR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "filtered_words = ['love', 'reading', 'books', 'technology', 'provide', 'great', 'way', 'learn', 'new', 'advancements', 'ideas', 'technology', 'made', 'lives', 'easier', 'connected']\n",
    "\n",
    "porter_stemmed = [porter_stemmer.stem(word) for word in filtered_words]\n",
    "lancaster_stemmed = [lancaster_stemmer.stem(word) for word in filtered_words]\n",
    "\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "print(\"Filtered Words:\", filtered_words)\n",
    "print(\"Porter Stemmed:\", porter_stemmed)\n",
    "print(\"Lancaster Stemmed:\", lancaster_stemmed)\n",
    "print(\"Lemmatized:\", lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b125e743-584b-43c7-bc4c-580d4078077c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reading', 'technology', 'provide', 'advancements', 'technology', 'easier', 'connected']\n",
      "[]\n",
      "[]\n",
      "['i', 'love', 'reading', 'books', 'about', 'technology', 'they', 'provide', 'a', 'great', 'way', 'to', 'learn', 'about', 'new', 'advancements', 'and', 'ideas', 'technology', 'has', 'made', 'our', 'lives', 'easier', 'and', 'more', 'connected']\n",
      "['i', 'about', 'a', 'about', 'advancements', 'and', 'ideas', 'our', 'easier', 'and']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "words_more_than_5 = re.findall(r'\\b\\w{6,}\\b',text)\n",
    "\n",
    "numbers = re.findall(r'\\b\\d+\\b',text)\n",
    "\n",
    "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b' , text)\n",
    "\n",
    "words_alpha = re.findall(r'\\b[a-zA-Z]+\\b' , text)\n",
    "\n",
    "words_starting_vowel = re.findall(r'\\b[aeiouAEIOU]\\w*', text)\n",
    "\n",
    "print(words_more_than_5)\n",
    "print(numbers)\n",
    "print(capitalized_words)\n",
    "print(words_alpha)\n",
    "print(words_starting_vowel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db780d73-dd1b-478c-a69a-c3f41434048d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text: Contact me at <EMAIL> or visit <URL> Call me at <PHONE>.\n",
      "Custom Tokens: ['Contact', 'me', 'at', 'exampleexamplecom', 'or', 'visit', 'httpsexamplecom', 'Call', 'me', 'at', '91', '9876543210']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def custom_tokenize(text):\n",
    "    text = re.sub(r'[^\\w\\s\\']', '', text)\n",
    "    text = re.sub(r'\\b\\w+\\-\\w+\\b', lambda x: x.group(0), text)\n",
    "    # Tokenize\n",
    "    return text.split()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b', '<EMAIL>', text)  # Email regex\n",
    "    text = re.sub(r'http[s]?://\\S+', '<URL>', text)  # URL regex\n",
    "    text = re.sub(r'\\+?\\d{1,3}\\s?\\(?\\d{1,4}?\\)?\\s?\\d{1,4}[-\\s]?\\d{1,4}[-\\s]?\\d{1,4}', '<PHONE>', text)  # Phone regex\n",
    "    return text\n",
    "\n",
    "text_with_contacts = \"Contact me at example@example.com or visit https://example.com. Call me at +91 9876543210.\"\n",
    "\n",
    "cleaned_text = clean_text(text_with_contacts)\n",
    "print(\"Cleaned Text:\", cleaned_text)\n",
    "\n",
    "tokens = custom_tokenize(text_with_contacts)\n",
    "print(\"Custom Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad294b66-5993-4408-a29f-196efcd44134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
